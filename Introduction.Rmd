---
title: "Introduction"
author: "Tim König"
date: "2023-02-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
{
  library(tidyverse)
  library(tuber) # this potentially needs replacing with my last pull request if the bug fix is not implemented until then
  library(lubridate)
  library(igraph)
  library(ggraph)
  library(quanteda)
  library(quanteda.textstats)
  library(stm)
}

yt_oauth("81481321824-btqvdmev607hbbq3n34i4qpb194spdq7.apps.googleusercontent.com", 
         "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")

get_related_related_videos <- function(video_ids, max_results, ...) { # ... passes additional arguments to get_related_video
  result <- tibble() # it might be more efficient to set the cols and expected rows of the container here (and delete empty rows later)
  for (id in video_ids) {
    videos <- get_related_videos(video_id = id, max_results = max_results, ...)
    result <- result %>% bind_rows(videos)
  }
  return(result)
}

party_videos <- readRDS("data/party_videos.RDS")
comments_january <- readRDS("data/comments_january.RDS")


```


# Videos

## Youtube Kanäle der Parteien


Channel IDs
```{r}
party_channels <- tibble(
  party = c("cdu",
            "spd",
            "gruene",
            "fdp", 
            "afd", 
            "linke"),
  channel_id = c("UCKyWIEse3u7ExKfAWuDMVnw",
                 "UCSmbK1WtpYn2sOGLvSSXkKw",
                 "UC7TAA2WYlPfb6eDJCeX4u0w",
                 "UC-sMkrfoQDH-xzMxPNckGFw",
                 "UCq2rogaxLtQFrYG3X3KYNww",
                 "UCA95T5bSGxNOAODBdbR2rYQ")
)

print(party_channels)
```

## Videos der Kanäle

```{r}

party_videos <- tibble() # empty container object to fill

for (i in 1:nrow(party_channels)) { # loop through every row in the dataframe of party channels
  
  videos <-  list_channel_videos(channel_id = party_channels$channel_id[i], # make the API call
                                 max_results = Inf)
  
  videos <- videos %>% mutate(party = party_channels$party[i]) # add party indicator

  party_videos <- bind_rows(party_videos, videos) # bind results
  
  cat(party_channels$party[i], ": ", nrow(videos), " videos retrieved \n", sep = "") # some printout to keep track
  
  Sys.sleep(3) # a sleep period between calls to avoid API errors
  
}

saveRDS(party_videos, "data/party_videos.RDS") # safe the data retrieved from the API

```

## Überblickstatistiken

```{r, echo = F}
party_videos <- party_videos %>% # some data cleaning
  mutate(contentDetails.videoPublishedAt = ymd_hms(contentDetails.videoPublishedAt)) %>%  # proper datetime format
  distinct(contentDetails.videoId, .keep_all = T) %>% # make sure there are no duplicates
  mutate(party_label = case_when( # add some proper party labels
    party == "afd" ~ "AfD",
    party == "cdu" ~ "CDU",
    party == "spd" ~ "SPD",
    party == "fdp" ~ "FDP",
    party == "gruene" ~ "Bündnis 90/Die Grünen",
    party == "linke" ~ "DIE LINKE"
  ))

party_videos %>% 
  group_by(party_label) %>% # grouping the analyses results by party
  summarise(nr_videos = n(), # count entries
            first_video = min(contentDetails.videoPublishedAt)) %>%  # lowest value = first published video
  arrange(desc(nr_videos)) %>%  # order in descending order of videos posted
  print()

colors <-
  tibble(
    Partei = c(
      "AfD",
      "CDU",
      "CSU",
      "DIE LINKE",
      "FDP",
      "Bündnis 90/Die Grünen",
      "SPD"
    ),
    # a tibble storing the correct party colors
    Farbe = c(
      rgb(0, 60, 145, maxColorValue = 255),
      rgb(50, 48, 46, maxColorValue = 255),
      rgb(98, 180, 255, maxColorValue = 255),
      rgb(182, 28, 62, maxColorValue = 255),
      rgb(255, 237, 0, maxColorValue = 255),
      rgb(70, 150, 43, maxColorValue = 255),
      rgb(227, 0, 15, maxColorValue = 255)
    )
  )

party_videos %>% mutate(month = floor_date(contentDetails.videoPublishedAt, unit = "month")) %>% # make month indicator
  group_by(party_label, month) %>% reframe(videos_published = n()) %>%  # aggregate by party and month
  ggplot() +                                                            # initiate ggplot on the wrangled data
  geom_line(aes(x = month, y = videos_published, color = party_label)) +   # draw a line plot
  scale_color_manual(values = pivot_wider(colors, names_from = Partei, values_from = Farbe) %>% unlist()) + # add the manual party colors
  facet_wrap( ~ party_label, ncol = 3) +                              # facet by party (each has its own plot)
  labs(                                                               # make some proper labels to improve readability for audiences
    x = "Monat",             
    y = "Veröffentlichte Videos",
    color = "Partei",
    title = "Videos über Zeit nach Partei, Gesamtzeitraum"
  ) +
  theme_bw()

party_videos %>% mutate(week = floor_date(contentDetails.videoPublishedAt, unit = "week")) %>% # make week indicator (more fine-grained than month)
  group_by(party_label, week) %>% reframe(videos_published = n()) %>%  # aggregate by party and week
  filter(week >= ymd("2021-01-01")) %>%                                # let's take a closer look at the timeline from 2021 onwards via filterin data
  ggplot() +                                                            # initiate ggplot on the wrangled data
  geom_line(aes(x = week, y = videos_published, color = party_label)) +   # draw a line plot
  scale_color_manual(values = pivot_wider(colors, names_from = Partei, values_from = Farbe) %>% unlist()) + # add the manual party colors
  scale_x_datetime(guide = guide_axis(angle = 30)) + # angle the labels on the X axis for better readability
  facet_wrap( ~ party_label, ncol = 3) +             # facet by party (each has its own plot)
  labs(                                              # make some proper labels to improve readability for audiences
    x = "Woche",
    y = "Veröffentlichte Videos",
    color = "Partei",
    title = "Videos über Zeit nach Partei, ab 2021"
  ) +
  theme_bw()

party_videos %>% mutate(month = floor_date(contentDetails.videoPublishedAt, unit = "month")) %>% # make month indicator (for higher aggregation than week)
  group_by(month) %>% reframe(videos_published = n()) %>%  # aggregate by mnth only, across all parties
  filter(month >= ymd("2021-01-01")) %>%                                # let's take a closer look at the timeline from 2021 onwards via filtering date
  ggplot() +                                                            # initiate ggplot on the wrangled data
  geom_point(aes(x = month, y = videos_published, size = videos_published)) +   # draw a point plot
  geom_line(aes(x = month, y = videos_published)) +                             # draw a line plot over it. Note how there is no color argument, as we do no distinguish parties
  scale_x_datetime(guide = guide_axis(angle = 30), breaks = "3 month") + # angle the labels on the X axis for better readability, set label breaks every 3 months
  labs(                                                                  # make some proper labels to improve readability for audiences
    x = "Monat",
    y = "Veröffentlichte Videos",
    size = "",
    title = "Videos über Zeit gesamt, ab 2021"
  ) +
  theme_bw()

## Note how the green party did not post a video since October last year
## Note how there is a visible peak during the federal elections, but there are other peaks as well, e.g. in January 2022

```

# Youtube-Kommentare zu Parteivideos, Januar 2022
```{r}
videos_january <- party_videos %>% filter(contentDetails.videoPublishedAt >= ymd("2022-01-01"), contentDetails.videoPublishedAt < ymd("2022-02-01")) # filter by publication dates

videos_january %>% group_by(party_label) %>% summarise(videos = n())
```

```{r}

comments_january <- tibble() # empty container

for (i in videos_january$contentDetails.videoId) { 
  # note how we can loop directly over the contents of this list, rather than using i in 1:nrow(videos_january) and accessing the content later through videos_january$contentDetails.videoId[i]
  
  comments <- NULL # this is only necessary to indicate missing / erroneous API calls correctly (otherwise we could just overwrite it every loop)
  
  cat(i, "\n")
  
  
  try({  # wrapping the call in try() makes sure the loop continues when an API call fails. Wrapping bind_rows() as well _after_ the call makes sure rows are only bound if comments are retrieved
    
      comments <- get_all_comments(video_id = i) # make API call
    
      comments_january <- bind_rows(comments_january, comments) # bind results
      
  })
  
  if(!is.null(comments)) {cat(nrow(comments), "comments retrieved\n")} # some output to keep track
  else {cat("no results \n")}
    
  Sys.sleep(1) # a sleep period between calls to avoid API errors
}

## Note that this retrieves only top-level comments, no threads


saveRDS(comments_january, "data/comments_january.RDS") # safe the data retrieved from the API

```

```{r}
party_comments_january <- left_join(videos_january, 
                                    comments_january %>% # join the comments on the video list...
                                      select(videoId, textDisplay, textOriginal, authorDisplayName, likeCount, publishedAt, id) %>%  # ... but select relevant data first ...
                                      rename(comment_id = id),    # ... and rename the ID variable for clarity
                                    by = join_by(contentDetails.videoId == videoId), # we need to specify that the IDs have different column names in the two datasets
                                    multiple = "all") %>%  # we expect multiple matches from the right-hand side (comments_january) to the left-hand side (videos_january)
  mutate(likeCount = as.integer(likeCount),              # count as integer, not character
         publishedAt = ymd_hms(publishedAt)) %>%       # proper dateteime format
  rename(video_id = contentDetails.videoId,              # rename some variables for clarity
         video_PublishedAt = contentDetails.videoPublishedAt,
         comment_PublishedAt = publishedAt) %>% 
  select(!c(kind, etag, id))                             # drop unnecessary variables

print(party_comments_january)

cat("Missing comments for", # some printout to check the number of videos with missing comments
    party_comments_january %>% filter(is.na(comment_id)) %>% nrow(), # count number of rows with missing comment_id, effectively one for each video without comments
    "out of",
    party_comments_january %>% distinct(video_id) %>% nrow(), # count total number of videos in the dataset (incl. those with missing comments)
    "videos.") # note that cat() is only used for the printout here and not strictly necessary

party_comments_january %>% group_by(party_label) %>% 
  count(!is.na(comment_id), .drop = F) %>%                               # count non-missing and missing comments
  pivot_wider(names_from = `!is.na(comment_id)`, values_from = n) %>%    # wrangle into wide format
  rename(comments = "TRUE", videos_without_comments = "FALSE") %>%       # rename variables for clarity
  replace_na(list(comments = 0, videos_without_comments = 0)) %>%        # replace NA values with 0
  left_join(videos_january %>% group_by(party_label) %>% summarise(total_videos = n()), by = "party_label") %>%  # add total number of videos through another dataframe
  mutate(average_comments_per_video = comments / total_videos)           # calculate average comments per video (this is a good engagement measure!)

# a closer look at youtube.com reveals that: 
#     a) the Green party has comments deactivated
#     b) it seems nobody cared to comment on videos by the FDP

```
## Quanteda

https://quanteda.io/articles/quickstart.html

```{r}
corpus <- corpus(party_comments_january %>%  # make a quanteda corpus
                   filter(!is.na(comment_id)), # remove missing comments 
                 docid_field = "comment_id",   # assign docid
                 text_field = "textOriginal")  # assign text; docvars are saved automatically

print(corpus)

tokens <- tokens(corpus,            # extract word tokens and remove punctuation
                 what = "word",
                 remove_punct = T)

print(tokens)


dfm <- dfm(tokens,       # construct document-feature-matrix, all words to lower casing
           tolower = T) %>% 
  dfm_remove(stopwords("german")) %>%  # remove german stopwords
  dfm_wordstem(language = "de")        # stem the words

print(dfm)
```

```{r}
tokens %>% tokens_group(party_label) %>% kwic("deutschland")
```



```{r}
dfm %>% dfm_group(party_label) %>% dfm_sort()

# get top tokens for each party

dfm %>% textstat_frequency(groups = party_label) %>%
  group_by(group) %>% slice_max(docfreq, n = 10)      # top tokens by doc frequency (occurrence in unique documents) rather than total occurrence


# similarity and distance

# tstat_dist <- dfm %>% dfm_group(video_id) %>% dfm_weight(scheme = "prop") %>% textstat_dist() # hierarchical clustering - get distances on normalized dfm
# 
# cluster <- hclust(as.dist(tstat_dist))  # hiarchical clustering the distance object
# 
# cluster$labels <- paste0(dfm %>% dfm_group(video_id) %>% docvars("party"), "_", dfm %>% dfm_group(video_id) %>% docnames()) # label with party names
# 
# # plot as a dendrogram
# plot(cluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")


tstat_dist <- dfm %>% dfm_group(party_label) %>% dfm_weight(scheme = "prop") %>% textstat_dist(method = "euclidean") # hierarchical clustering - get distances on normalized dfm

cluster <- hclust(as.dist(tstat_dist))  # hiarchical clustering the distance object

cluster$labels <- dfm %>% dfm_group(party_label) %>% docnames() # label 

# plot as a dendrogram
plot(cluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")



tstat_simi<- dfm %>% dfm_group(party_label) %>% dfm_weight(scheme = "prop") %>% textstat_simil(method = "cosine") # hierarchical clustering - get distances on normalized dfm

cluster_simi <- hclust(as.dist(tstat_simi))  # hiarchical clustering the distance object

cluster_simi$labels <- dfm %>% dfm_group(party_label) %>% docnames() # label 

# plot as a dendrogram
plot(cluster_simi, xlab = "", sub = "", main = "Cosine Similarity on Normalized Token Frequency")

```
# Structural Topic Modeling
```{r}
out <- convert(dfm, to = "stm")


Ks_prevalence <- searchK(documents = out$documents, vocab = out$vocab, 
                         K = c(5:100),# Number of topics for diagnostics (each K is a model) [this is excessive and would take a while to process on a normal machine]
                         prevalence = ~party, data = out$meta,               # ~ is only in the beggining of the equation since it seperates left and right hand side!
                         gamma.prior = "L1", init.type = "Spectral",
                         cores = 20)

saveRDS(Ks_prevalence, "results/K_tests.RDS")

```

```{r}

Ks_prevalence <- readRDS("results/K_tests.RDS")

plot.searchK(Ks_prevalence)

ggplot(Ks_prevalence$results) +
  geom_point(aes(x=as.double(exclus), y = as.double(semcoh), color = as.integer(K))) +
  labs(x = "Exclusivity", y = "Semantic Coherence", color = "K")

ggplot(Ks_prevalence$results %>% filter(K <= 20)) +
  geom_point(aes(x=as.double(exclus), y = as.double(semcoh), color = as.character(K))) +
    labs(x = "Exclusivity", y = "Semantic Coherence", color = "K")
```


```{r}
stm <- stm(documents = out$documents, vocab = out$vocab, # actual topic model
            K = 6,
            prevalence = ~party, data = out$meta,               
            gamma.prior = "L1", init.type = "Spectral")

saveRDS(stm, "results/stm_k6.RDS")
```


